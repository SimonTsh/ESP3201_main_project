# -*- coding: utf-8 -*-
"""efficient_net_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SIBEO2j_pmg68n1M4n9S8PmxxXousZiz

# Setting up Data and Packages

## Import necessary packages
"""

import os, re, math

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns

import cv2
import tensorflow as tf

"""## Download data
The first step is to download the training data and the test data

"""

from google.colab import drive, files

drive.mount("/content/gdrive", force_remount=True)
!ls '/content/gdrive/My Drive/datasets'

"""Altneratively, mount the Google Drive

## Loading the CSV Files
"""

df = pd.read_csv("/content/gdrive/My Drive/datasets/drive_data.csv")
df = df.iloc[::3, :] # keeping only results from every 3rd index
print(len(df))
df.head()
# df_val = pd.read_csv("data/val/val.csv")
# df_test = pd.read_csv("data/test/test.csv")

# to split csv file into training and validation sets (80:20)
df_val = df.iloc[:(len(df)//4), :]
print(len(df_val))
df_train = df.iloc[(len(df)//4):, :]
print(len(df_train))
df_train.head()

# define variables used
frame_count = 0
time_increment = 30 # taking at every index
frame_increment = time_increment / 30

# vid = cv2.VideoCapture('/content/gdrive/My Drive/datasets/outpy_train.mp4')
vid = cv2.VideoCapture('/content/gdrive/My Drive/datasets/6min_scnn_labeled.mp4')
success, image = vid.read()

while success and frame_count < len(df):
  if frame_count % frame_increment == 0:

    cv2.imwrite("frame%d_raw_1.png" % frame_count, image)

  success, image = vid.read()
  if frame_count % 100 == 0:
      print("Count: ", frame_count)
  frame_count += 1

"""# Exploratory Data Analysis
We now see the kind of images the dataset contains to get a better idea.

## First look at the Originals
"""

Y_SIZE, X_SIZE = 4, 6
ROWS, COLS = 1, 1
fig, ax = plt.subplots(ROWS, COLS, figsize=(COLS * X_SIZE, ROWS * Y_SIZE))
ax = np.reshape(ax, (-1,))
i = 0
for idx, data in df_val.head(ROWS).iterrows():
    filename, label = i, data['angle']
    print(filename, label, idx)
    image = plt.imread("/content/frame%d_raw_1.png"%(filename))
    width, height, RGB_channels = image.shape
    # print(width, height, RGB_channels)

    ax[int(idx/3)].imshow(image)
    ax[int(idx/3)].set_title('Angle: %f'%(label))
    # ax[int(idx/3)].set_title('Index: %f'%(filename))
    i += 1
  
# j = len(df_val) # starting point is where training data ends
# for idx, data in df_train.head(ROWS).iterrows():
#     filename, label = j, data['angle']
#     print(filename, label, idx)
#     image = plt.imread("/content/frame%d_raw_1.png"%(filename))
    
#     ax[int(idx/j)].imshow(image)
#     ax[int(idx/j)].set_title('Angle: %f'%(label))
#     # ax[int(idx/j)].set_title('Index: %f'%(filename))
#     j += 3

"""## Trying some filters on"""

def transform(image):
    return cv2.Canny(image[75:, :], 100, 200)

Y_SIZE, X_SIZE = 4, 6
ROWS, COLS = 2, 4
fig, ax = plt.subplots(ROWS, COLS, figsize=(COLS * X_SIZE, ROWS * Y_SIZE))
ax = np.reshape(ax, (-1,))
i = 0
for idx, data in df_val.head(ROWS).iterrows():
    filename, label = i, data['angle']
    print(filename, label, idx)
    image = transform((plt.imread("/content/frame%d_raw_1.png"%(filename))*255).astype(np.uint8))
    width, height = image.shape # no more RGB channels
    print(width, height)
    ax[int(idx/3)].imshow(image)
    ax[int(idx/3)].set_title('Angle: %f'%(label))
    # ax[int(idx/3)].set_title('Index: %f'%(filename))
    i += 1
  
j = len(df_val) # starting point is where training data ends
for idx, data in df_train.head(ROWS).iterrows():
    filename, label = j, data['angle']
    print(filename, label, idx)
    image = transform((plt.imread("/content/frame%d_raw_1.png"%(filename))*255).astype(np.uint8))
    
    ax[int(idx/j)].imshow(image)
    ax[int(idx/j)].set_title('Angle: %f'%(label))
    # ax[int(idx/j)].set_title('Index: %f'%(filename))
    j += 3

"""## Plot the Steering Angle Distribution"""

fig, ax = plt.subplots(1, 2, figsize=(14, 5))
sns.distplot(df_train.angle, ax=ax[0], color='IndianRed')
sns.distplot(df_val.angle, ax=ax[1], color='RoyalBlue')
plt.show()

"""# TFRecord Generation

## Hyperparameter Setup
"""

BATCH_SIZE = 4 #@param {type:"number"}
IMAGE_SIZE = 224 #@param {type:"number"}
iMAGE_WIDTH = 1024 #@param {type:"number"}
IMAGE_HEIGHT = 2048 #@param {type:"number"}
RGB_CHANNELS = 3 #@param {type:"number"}

"""## Load Data
We use PIL library to load our images. Here we are creating our array where our input features are the mean colours and output features are the rotations along the x axis.
"""

print('Number of rows in train:', len(df_train))
print('Number of rows in val:', len(df_val))
# print('Number of rows in test:', len(df_test))

def get_image(filename):
    # image = transform((plt.imread("/content/frame%d_raw_1.png"%(filename))*255).astype(np.uint8))
    # image = transform(cv2.imread('/content/frame%d_raw_1.png'%(filename)))
    image = cv2.imread('/content/frame%d_raw_1.png'%(filename))
    # image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))
    return cv2.imencode('.jpg', image, (cv2.IMWRITE_JPEG_QUALITY, 100))[1].tostring()

def get_label(value):
    return value

def round_sig(x, sig):
    return round(x, sig-int(math.floor(math.log10(abs(x))))-1)

# validation set
RECORD_SIZE = 2000
RECORD_COUNT = (len(df_val) + RECORD_SIZE - 1) // RECORD_SIZE
print(RECORD_COUNT)

for j in range(RECORD_COUNT):
    print('Writing Record %i of %i...' % (j + 1, RECORD_COUNT), end=' ')
    CURRENT_RECORD_SIZE = min(RECORD_SIZE, len(df_val) - j * RECORD_SIZE)
    print(CURRENT_RECORD_SIZE, len(df_val))
    with tf.io.TFRecordWriter('autodri_val_%d.tfrec'%(j)) as writer:
        for k in range(CURRENT_RECORD_SIZE):
            index = j * RECORD_SIZE + k
            filename = int(df_val.iloc[index]['idx']/3) # shifting index to /3
            # print(filename)
            # filename = "frame%d_raw_1.png"%(df_val.iloc[j * RECORD_SIZE + k])
            angle = df_val.iloc[index]['angle']
            feature = {
                # image = plt.imread("/content/frame%d_raw_1.png"%(filename))
                'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[get_image(filename)])),
                'angle': tf.train.Feature(float_list=tf.train.FloatList(value=[get_label(angle)]))
            }
            example_proto = tf.train.Example(features=tf.train.Features(feature=feature))
            writer.write(example_proto.SerializeToString())
            if (k % 100 == 0): print(k // 100, end=' ')
    print()

# training set
RECORD_SIZE = 2000
RECORD_COUNT = (len(df_train) + RECORD_SIZE - 1) // RECORD_SIZE
print(RECORD_COUNT)

for j in range(RECORD_COUNT):
    print('Writing Record %i of %i...' % (j + 1, RECORD_COUNT), end=' ')
    CURRENT_RECORD_SIZE = round_sig(min(RECORD_SIZE, len(df_train) - j * RECORD_SIZE), 2)
    # CURRENT_RECORD_SIZE = min(RECORD_SIZE, len(df_train) - j * RECORD_SIZE)
    print(CURRENT_RECORD_SIZE, len(df_train))
    with tf.io.TFRecordWriter('autodri_train_%d.tfrec'%(j)) as writer:
        for k in range(CURRENT_RECORD_SIZE):
            index = j * RECORD_SIZE + k
            filename = int(df_train.iloc[index]['idx']/3) # shifting index to /3
            # print(filename)
            # filename = "frame%d_raw_1.png"%(df_train.iloc[j * RECORD_SIZE + k])
            angle = df_train.iloc[index]['angle']
            feature = {
                # image = plt.imread("/content/frame%d_raw_1.png"%(filename))
                'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[get_image(filename)])),
                'angle': tf.train.Feature(float_list=tf.train.FloatList(value=[get_label(angle)]))
            }
            example_proto = tf.train.Example(features=tf.train.Features(feature=feature))
            writer.write(example_proto.SerializeToString())
            if (k % 100 == 0): print(k // 100, end=' ')
    print()

if os.path.exists('tfrec'):
  !rm -rf tfrec

if not os.path.exists('tfrec'):
    os.mkdir('tfrec')
    os.mkdir('tfrec/train')
    os.mkdir('tfrec/val')
    # os.mkdir('tfrec/test')
    for filename in os.listdir():
        if 'autodri_train' in filename:
            os.rename(filename, 'tfrec/train/' + filename)
        if 'autodri_val' in filename:
            os.rename(filename, 'tfrec/val/' + filename)
        # if 'autodri_test' in filename:
        #     os.rename(filename, 'tfrec/test/' + filename)

"""## Reading the TFRecords"""

def read_tfrecord(example):
    tfrec_format = {
        'image': tf.io.FixedLenFeature([], tf.string),
        'angle': tf.io.FixedLenFeature([], tf.float32),
    }
    example = tf.io.parse_single_example(example, tfrec_format)
    return example['image'], example['angle']

def prepare_image(img):
    img = tf.image.resize(tf.image.decode_jpeg(img, channels=3), (IMAGE_SIZE, IMAGE_SIZE))
    img = tf.cast(img, tf.float32) / 255.0
    return img

def get_dataset(files, repeat = False, train = True):
    ds = tf.data.TFRecordDataset(files, num_parallel_reads=tf.data.experimental.AUTOTUNE)
    ds = ds.cache()
    if repeat: ds = ds.repeat()
    ds = ds.map(read_tfrecord, num_parallel_calls=tf.data.experimental.AUTOTUNE)
    ds = ds.map(lambda image, label: tuple(
      [tuple([prepare_image(image)]), label]), 
      num_parallel_calls=tf.data.experimental.AUTOTUNE)
    ds = ds.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)
    return ds

"""## Looking at the TF-Record images"""

THUMBNAIL_SIZE = 10
ROWS, COLS = 1, 1

ds = get_dataset(['tfrec/train/' + filename for filename in os.listdir('tfrec/train/')]).unbatch().take(ROWS)
fig, ax = plt.subplots(ROWS, COLS, figsize=(COLS * THUMBNAIL_SIZE, ROWS * THUMBNAIL_SIZE))
ax = np.reshape(ax, (-1,))
for idx, data in enumerate(iter(ds)):
    images, label = data
    image = images
    # print(image, label)
    label = 'Angle: %f'%(label.numpy())
    ax[idx].imshow(image[0])
    ax[idx].set_title(label)

"""# Models and Predictions"""

fix, ax = plt.subplots(1, 2, figsize=(14, 5))
sns.distplot(df_train['angle'].apply(get_label), ax=ax[0])
sns.distplot(df_val['angle'].apply(get_label), ax=ax[1])
plt.show()

def get_model():
    image = tf.keras.layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3), name='input')
    network = tf.keras.applications.EfficientNetB5(include_top=False, weights='imagenet', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))
    steer = network(image)
    steer = tf.keras.layers.GlobalAveragePooling2D()(steer)
    steer = tf.keras.layers.Dropout(0.5)(steer)
    steer = tf.keras.layers.Dense(100, activation='relu', name='dense_1')(steer)
    steer = tf.keras.layers.Dense(10, activation='relu', name='dense_2')(steer)
    steer = tf.keras.layers.Dense(1, name='output')(steer)
    return tf.keras.models.Model(inputs=[image], outputs=[steer])

model = get_model()
model.summary()
tf.keras.utils.plot_model(model)

train_dataset = get_dataset(['tfrec/train/' + filename for filename in os.listdir('tfrec/train/')])
val_dataset = get_dataset(['tfrec/val/' + filename for filename in os.listdir('tfrec/val/')])
PROJECT_DIR = '/content/'
mc = tf.keras.callbacks.ModelCheckpoint(PROJECT_DIR + 'check_all4ori', monitor='val_loss', mode='min', save_best_only=True, save_weights_only=True)
lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.1, verbose=1)

model.compile(
    # loss=tf.keras.losses.Huber(name='loss_huber'), 
    loss = tf.keras.losses.MeanSquaredLogarithmicError(reduction="auto", name="mean_squared_logarithmic_error"),
    optimizer=tf.keras.optimizers.Adam(),
    metrics=[
        tf.keras.metrics.KLDivergence(name='metric_kl'),
        tf.keras.metrics.LogCoshError(name='metric_cosh'),
        tf.keras.metrics.MeanAbsoluteError(name='metric_mae'),
        tf.keras.metrics.MeanSquaredError(name='metric_mse'),
        tf.keras.metrics.MeanSquaredLogarithmicError(name='mean_squared_logarithmic_error'),
    ]
)

model.fit(
    train_dataset,
    epochs=10, 
    validation_data=val_dataset, 
    callbacks=[lr, mc]
)

model.fit(
    train_dataset,
    epochs=40, 
    validation_data=val_dataset, 
    callbacks=[lr, mc]
)

model.evaluate(train_dataset)

"""# Predict"""

checkpoint_path = "cp-{epoch:04d}.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)
model.save_weights(checkpoint_path.format(epoch=0))

latest = tf.train.latest_checkpoint(checkpoint_dir)
latest

!mkdir -p saved_model
model.save('saved_model/my_model_6')

model.load_weights(checkpoint_path.format(epoch=0))

ds = get_dataset(['tfrec/test/' + filename for filename in os.listdir('tfrec/test/')],train=False).unbatch().take(4)

results = model.predict(test_dataset)

df_train['angle'] = np.reshape(results, -1) # error message: Length of values (4) does not match length of index (12133)
df_train.to_csv('model_test_1.csv', index=False)

# download predicted weights result
files.download('model_test_1.csv')
